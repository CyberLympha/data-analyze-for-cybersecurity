{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT's ARCHITECTURE\n",
        "\n",
        "BERT (англ. Bidirectional Encoder Representations from Transformers) — языковая модель, основанная на архитектуре трансформер, предназначенная для предобучения языковых представлений с целью их последующего применения в широком спектре задач обработки естественного языка."
      ],
      "metadata": {
        "id": "OTNIyDtqjO-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Существует четыре типа предварительно обученных версий BERT в зависимости от масштаба архитектуры модели:\n",
        "\n",
        "- BERT-Base (Cased / Un-Cased): 12-layer, 768-hidden-nodes, 12-attention-heads, 110M parameters\n",
        "- BERT-Large (Cased / Un-Cased): 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters<\n",
        "\n",
        "<center><img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/09/bert_encoder.png\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "sszH_rVdjO-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocessing\n",
        "\n",
        "BERT опирается на Transformer (механизм внимания, который изучает контекстные связи между словами в тексте). Базовый Transformer состоит из кодировщика для чтения текстового ввода и декодера для создания прогноза для задачи. Поскольку цель BERT — создать модель представления языка, ему нужна только часть кодировщика. Входные данные для кодировщика для BERT — это последовательность токенов, которые сначала преобразуются в векторы, а затем обрабатываются в нейронной сети. Для начала, каждое вложение входных данных представляет собой комбинацию из 3 вложений:\n",
        "\n",
        "<center><img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/09/bert_emnedding.png\"></center>\n",
        "Входное представление для BERT: Входные вложения представляют собой сумму вложений токенов, вложений сегментации и вложений позиции.\n",
        "\n",
        "#2. <u>Pre-training Tasks</u></h1></center>\n",
        "\n",
        "## 1. Masked Language Modelling</h1>\n",
        "\n",
        "## 2. Next Sentence Prediction</h1>\n"
      ],
      "metadata": {
        "id": "BlTA03kDjO-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель обучается с помощью Masked LM и Next Sentence Prediction вместе. Это делается для минимизации объединенной функции потерь двух стратегий — «вместе лучше».\n"
      ],
      "metadata": {
        "id": "x2j1U51pjO-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# РЕАЛИЗАЦИЯ BERT"
      ],
      "metadata": {
        "id": "c9lA7gHpjO-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Постановка задачи**:Имеется коллекция SMS-сообщений. Некоторые из этих сообщений являются спамом, а остальные — подлинными.\n",
        "\n",
        "Задача — создать систему, которая автоматически определяла бы, является ли сообщение спамом или нет."
      ],
      "metadata": {
        "id": "ICFInHctjO-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">1. Import Required Libraries & Dataset</h1>"
      ],
      "metadata": {
        "id": "uza043x9jO-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:16:43.7677Z",
          "iopub.execute_input": "2021-05-20T12:16:43.768047Z",
          "iopub.status.idle": "2021-05-20T12:16:43.773166Z",
          "shell.execute_reply.started": "2021-05-20T12:16:43.767999Z",
          "shell.execute_reply": "2021-05-20T12:16:43.772221Z"
        },
        "trusted": true,
        "id": "Gw9vTBfzjO-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/spamdata_v2.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:16:45.214126Z",
          "iopub.execute_input": "2021-05-20T12:16:45.214458Z",
          "iopub.status.idle": "2021-05-20T12:16:45.244325Z",
          "shell.execute_reply.started": "2021-05-20T12:16:45.21443Z",
          "shell.execute_reply": "2021-05-20T12:16:45.243406Z"
        },
        "trusted": true,
        "id": "aT7GBD9ajO-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Набор данных состоит из двух столбцов – «label» и «text».\n",
        "\n",
        "Столбец «text» содержит тело сообщения, а «label» – это бинарная переменная, где 1 означает спам, а 0 означает, что сообщение не является спамом."
      ],
      "metadata": {
        "id": "Ox3hwpxbjO-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check class distribution\n",
        "df['label'].value_counts(normalize = True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:16:45.962743Z",
          "iopub.execute_input": "2021-05-20T12:16:45.963084Z",
          "iopub.status.idle": "2021-05-20T12:16:45.972824Z",
          "shell.execute_reply.started": "2021-05-20T12:16:45.963045Z",
          "shell.execute_reply": "2021-05-20T12:16:45.971725Z"
        },
        "trusted": true,
        "id": "b_ds2iG2jO-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">2. Split the Dataset into train / test</h1>"
      ],
      "metadata": {
        "id": "Ws3xKdjajO-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split train dataset into train, validation and test sets\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'],\n",
        "                                                                    random_state=2018,\n",
        "                                                                    test_size=0.3,\n",
        "                                                                    stratify=df['label'])\n",
        "\n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
        "                                                                random_state=2018,\n",
        "                                                                test_size=0.5,\n",
        "                                                                stratify=temp_labels)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:16:46.732938Z",
          "iopub.execute_input": "2021-05-20T12:16:46.733295Z",
          "iopub.status.idle": "2021-05-20T12:16:46.749012Z",
          "shell.execute_reply.started": "2021-05-20T12:16:46.733265Z",
          "shell.execute_reply": "2021-05-20T12:16:46.748078Z"
        },
        "trusted": true,
        "id": "dU4pf_rfjO-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Import Bert - base- uncased</h1>\n",
        "\n",
        "**bert-base-uncased** — это предобученная языковая модель семейства BERT (Bidirectional Encoder Representations from Transformers), разработанная Google Research. Эта версия модели имеет следующие характеристики.\n",
        "\n",
        "Основные особенности:\n",
        "- Размер: Базовая версия (base), содержащая 12 слоев трансформера.\n",
        "- Размеры скрытых представлений: 768 размерность.\n",
        "- Количество параметров: Приблизительно 110 миллионов.\n",
        "- Обучение: Обучалась на большом корпусе текста на английском языке, включая данные с Common Crawl и BooksCorpus.\n",
        "- Cased vs Uncased: Модель uncased, что означает, что текст перед обработкой приводится к нижнему регистру (все буквы становятся строчными). Это упрощает обработку и уменьшает влияние различий между заглавными и строчными буквами.\n",
        "  \n",
        "Применение:\n",
        "\n",
        "Эта модель используется для широкого спектра NLP-задач, таких как классификация текста, извлечение сущностей, машинный перевод, обобщение текста и другие задачи обработки естественного языка. Она доступна в библиотеках PyTorch и TensorFlow через платформы Hugging Face и других популярных фреймворков.\n"
      ],
      "metadata": {
        "id": "ugdg_O7kjO-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import BERT-base pretrained model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T11:49:24.186902Z",
          "iopub.execute_input": "2021-05-20T11:49:24.18724Z",
          "iopub.status.idle": "2021-05-20T11:50:15.137513Z",
          "shell.execute_reply.started": "2021-05-20T11:49:24.187212Z",
          "shell.execute_reply": "2021-05-20T11:50:15.135994Z"
        },
        "trusted": true,
        "id": "WcYV8lsljO-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получим длину сообщений в тренировочном наборе"
      ],
      "metadata": {
        "id": "0QJUeVNZsq8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:16:50.490321Z",
          "iopub.execute_input": "2021-05-20T12:16:50.490642Z",
          "iopub.status.idle": "2021-05-20T12:16:50.677576Z",
          "shell.execute_reply.started": "2021-05-20T12:16:50.490613Z",
          "shell.execute_reply": "2021-05-20T12:16:50.676797Z"
        },
        "trusted": true,
        "id": "tjTKJLqrjO-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">4. Tokenize & Encode the Sequences</h1>"
      ],
      "metadata": {
        "id": "7YPeUqKRjO-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT использует токенизатор WordPiece. Словарь инициализируется всеми отдельными символами языка, а затем итеративно добавляются наиболее частые/вероятные комбинации существующих слов в словаре.\n",
        "\n",
        "Максимальная длина последовательности ввода = 512"
      ],
      "metadata": {
        "id": "gI4hA_vdjO-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = 25,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:16:51.367515Z",
          "iopub.execute_input": "2021-05-20T12:16:51.367829Z",
          "iopub.status.idle": "2021-05-20T12:16:52.037957Z",
          "shell.execute_reply.started": "2021-05-20T12:16:51.3678Z",
          "shell.execute_reply": "2021-05-20T12:16:52.037114Z"
        },
        "trusted": true,
        "id": "zFRSEF7ajO-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">5. List to Tensors</h1>"
      ],
      "metadata": {
        "id": "iU7WcNgXjO-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## convert lists to tensors\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:16:52.707085Z",
          "iopub.execute_input": "2021-05-20T12:16:52.707444Z",
          "iopub.status.idle": "2021-05-20T12:16:52.744645Z",
          "shell.execute_reply.started": "2021-05-20T12:16:52.707415Z",
          "shell.execute_reply": "2021-05-20T12:16:52.743449Z"
        },
        "trusted": true,
        "id": "FrrWYyROjO-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">6. Data Loader</h1>"
      ],
      "metadata": {
        "id": "uK0t_0SAjO-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 32\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:16:54.072724Z",
          "iopub.execute_input": "2021-05-20T12:16:54.073069Z",
          "iopub.status.idle": "2021-05-20T12:16:54.079387Z",
          "shell.execute_reply.started": "2021-05-20T12:16:54.073019Z",
          "shell.execute_reply": "2021-05-20T12:16:54.078289Z"
        },
        "trusted": true,
        "id": "HTo2k3xqjO-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">7. Model Architecture</h1>"
      ],
      "metadata": {
        "id": "PY2lC-DajO-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:16:55.343359Z",
          "iopub.execute_input": "2021-05-20T12:16:55.343696Z",
          "iopub.status.idle": "2021-05-20T12:16:55.349996Z",
          "shell.execute_reply.started": "2021-05-20T12:16:55.343669Z",
          "shell.execute_reply": "2021-05-20T12:16:55.348883Z"
        },
        "trusted": true,
        "id": "M_eDKtHcjO-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "\n",
        "        self.bert = bert\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,512)\n",
        "\n",
        "        # dense layer 2 (Output layer)\n",
        "        self.fc2 = nn.Linear(512,2)\n",
        "\n",
        "        #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "\n",
        "        #pass the inputs to the model\n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "\n",
        "        x = self.fc1(cls_hs)\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # output layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:23:02.99389Z",
          "iopub.execute_input": "2021-05-20T12:23:02.994288Z",
          "iopub.status.idle": "2021-05-20T12:23:03.006243Z",
          "shell.execute_reply.started": "2021-05-20T12:23:02.994244Z",
          "shell.execute_reply": "2021-05-20T12:23:03.005349Z"
        },
        "trusted": true,
        "id": "cmk7-eS1jO-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU/CPU\n",
        "device = torch.device(\"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:23:03.952902Z",
          "iopub.execute_input": "2021-05-20T12:23:03.953226Z",
          "iopub.status.idle": "2021-05-20T12:23:03.968064Z",
          "shell.execute_reply.started": "2021-05-20T12:23:03.953198Z",
          "shell.execute_reply": "2021-05-20T12:23:03.967316Z"
        },
        "trusted": true,
        "id": "AQ5uxKwDjO-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "#from transformers import AdamW\n",
        "from torch.optim import Adam\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = Adam(model.parameters(),lr = 1e-1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:23:04.903449Z",
          "iopub.execute_input": "2021-05-20T12:23:04.903784Z",
          "iopub.status.idle": "2021-05-20T12:23:04.910844Z",
          "shell.execute_reply.started": "2021-05-20T12:23:04.90375Z",
          "shell.execute_reply": "2021-05-20T12:23:04.909852Z"
        },
        "trusted": true,
        "id": "5xerfL5djO-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "print(\"Class Weights:\",class_weights)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:23:05.840224Z",
          "iopub.execute_input": "2021-05-20T12:23:05.840566Z",
          "iopub.status.idle": "2021-05-20T12:23:05.849927Z",
          "shell.execute_reply.started": "2021-05-20T12:23:05.840537Z",
          "shell.execute_reply": "2021-05-20T12:23:05.84887Z"
        },
        "trusted": true,
        "id": "8uVS-d1PjO-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# push to GPU/CPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights)\n",
        "\n",
        "# number of training epochs\n",
        "epochs = 2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:23:06.96215Z",
          "iopub.execute_input": "2021-05-20T12:23:06.96248Z",
          "iopub.status.idle": "2021-05-20T12:23:06.967108Z",
          "shell.execute_reply.started": "2021-05-20T12:23:06.962452Z",
          "shell.execute_reply": "2021-05-20T12:23:06.966225Z"
        },
        "trusted": true,
        "id": "0LlnhnPHjO-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">8. Fine - Tune (Тонкая настройка)</h1>"
      ],
      "metadata": {
        "id": "ZlW7cqPgjO-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to train the model\n",
        "def train():\n",
        "\n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    # empty list to save model predictions\n",
        "    total_preds=[]\n",
        "\n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(train_dataloader):\n",
        "\n",
        "        # progress update after every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [r.to(device) for r in batch]\n",
        "\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        # clear previously calculated gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # get model predictions for the current batch\n",
        "        preds = model(sent_id, mask)\n",
        "\n",
        "        # compute the loss between actual and predicted values\n",
        "        loss = cross_entropy(preds, labels)\n",
        "\n",
        "        # add on to the total loss\n",
        "        total_loss = total_loss + loss.item()\n",
        "\n",
        "        # backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # model predictions are stored on GPU/CPU\n",
        "        preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "      # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    #returns the loss and predictions\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:23:08.232922Z",
          "iopub.execute_input": "2021-05-20T12:23:08.233269Z",
          "iopub.status.idle": "2021-05-20T12:23:08.241591Z",
          "shell.execute_reply.started": "2021-05-20T12:23:08.233239Z",
          "shell.execute_reply": "2021-05-20T12:23:08.240484Z"
        },
        "trusted": true,
        "id": "ExGdoFY5jO-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "\n",
        "    print(\"\\nEvaluating...\")\n",
        "\n",
        "    # deactivate dropout layers\n",
        "    model.eval()\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "\n",
        "    # empty list to save the model predictions\n",
        "    total_preds = []\n",
        "\n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(val_dataloader):\n",
        "\n",
        "        # Progress update every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [t.to(device) for t in batch]\n",
        "\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        # deactivate autograd\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # model predictions\n",
        "            preds = model(sent_id, mask)\n",
        "\n",
        "            # compute the validation loss between actual and predicted values\n",
        "            loss = cross_entropy(preds,labels)\n",
        "\n",
        "            total_loss = total_loss + loss.item()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    # compute the validation loss of the epoch\n",
        "    avg_loss = total_loss / len(val_dataloader)\n",
        "\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:23:09.063641Z",
          "iopub.execute_input": "2021-05-20T12:23:09.063944Z",
          "iopub.status.idle": "2021-05-20T12:23:09.073677Z",
          "shell.execute_reply.started": "2021-05-20T12:23:09.063916Z",
          "shell.execute_reply": "2021-05-20T12:23:09.070943Z"
        },
        "trusted": true,
        "id": "-VKMsTEcjO-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "\n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "\n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:23:09.896772Z",
          "iopub.execute_input": "2021-05-20T12:23:09.897241Z",
          "iopub.status.idle": "2021-05-20T12:24:20.56839Z",
          "shell.execute_reply.started": "2021-05-20T12:23:09.897201Z",
          "shell.execute_reply": "2021-05-20T12:24:20.567287Z"
        },
        "trusted": true,
        "id": "qXmLnwxZjO-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:24:47.832372Z",
          "iopub.execute_input": "2021-05-20T12:24:47.832686Z",
          "iopub.status.idle": "2021-05-20T12:24:48.147778Z",
          "shell.execute_reply.started": "2021-05-20T12:24:47.832657Z",
          "shell.execute_reply": "2021-05-20T12:24:48.147016Z"
        },
        "trusted": true,
        "id": "poGBIsMcjO-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"font-size:200%; font-family:cursive; color:navy;\">9. Make Predictions</h1>"
      ],
      "metadata": {
        "id": "fS7_NQ96jO-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "    preds = model(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:25:07.435685Z",
          "iopub.execute_input": "2021-05-20T12:25:07.436008Z",
          "iopub.status.idle": "2021-05-20T12:25:08.019816Z",
          "shell.execute_reply.started": "2021-05-20T12:25:07.435979Z",
          "shell.execute_reply": "2021-05-20T12:25:08.018976Z"
        },
        "trusted": true,
        "id": "cbas1BqBjO-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-05-20T12:25:23.838389Z",
          "iopub.execute_input": "2021-05-20T12:25:23.838719Z",
          "iopub.status.idle": "2021-05-20T12:25:23.852193Z",
          "shell.execute_reply.started": "2021-05-20T12:25:23.838691Z",
          "shell.execute_reply": "2021-05-20T12:25:23.850795Z"
        },
        "trusted": true,
        "id": "jpGu-if5jO-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x0ssfsGNjO-k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}